### LAB1
setting up environments and install packages, then get familiar with the foundamental image processions.
- Obstacles: environments set up and workspace choosing.

### LAB2
learn basic functions of opencv including read, show image, draw rectangle/circle, write words, cropping image, write to local.
then haar-cascade classifer, able to detect edges of objects and return the range of it. NMS algorithm is applied to reduce the overlapping parts.   

**OpenCV 基础操作学习**

#### 📸 1. 图像读取与显示
- **读取**：
  - `cv2.imread(path)`：支持绝对路径或相对路径。
  - 返回值是 `numpy.ndarray`（n维数组），形状为 `(H, W, C)`（高、宽、通道（即RGB图和灰度图））。
  - 用 `img.shape` 查看图像尺寸，用 `img.dtype` 查看数据类型（通常是 uint8（无符号8位整数），取值范围 [0, 255]）。
```python
# 获取图像高度和宽度
h, w = img.shape[:2]

# 将图像转为灰度（如果原来是彩色）
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# 裁剪图像
cropped = img[100:300, 200:400]

# 修改某个像素的颜色
img[50, 100] = [0, 255, 0]  # 设为绿色
```
```python
import cv2

# 读取彩色图像
img = cv2.imread('cat.jpg')  # shape: (480, 640, 3)

# 分离通道
b, g, r = cv2.split(img)

# b.shape: (480, 640) —— 蓝色通道
# g.shape: (480, 640) —— 绿色通道
# r.shape: (480, 640) —— 红色通道

# 合并通道
merged = cv2.merge([b, g, r])
```
- **显示**：
  - `cv2.imshow('title', img)`：创建一个窗口显示图像。
  - `cv2.waitKey(0)`：等待任意键按下，否则窗口会立即关闭。
- ✅ **关键**：确保路径正确，图像文件存在。

#### 🖍️ 2. 图像基本操作
- **绘制矩形**：
  - `cv2.rectangle(img, (x1,y1), (x2,y2), color, thickness)`
  - 用于标注目标区域。
- **添加文字**：
  - `cv2.putText(img, text, org, font, scale, color, thickness)`
  - 用于在图像上添加说明。
- **裁剪与缩放**：
  - **裁剪**：使用 NumPy 的切片 `img[y1:y2, x1:x2]`。
  - **缩放**：`cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)`
  - `INTER_LINEAR` 是默认的插值方法，适合一般用途。

#### 💾 3. 图像保存
- **保存**：
  - `cv2.imwrite('filename.jpg', img)`：将处理后的图像保存到本地。
  - 文件名后缀决定保存格式（如 `.jpg`, `.png`）。

#### 🧩 4. 核心概念：图像即数组
- **本质**：OpenCV 中的图像是一个 NumPy 数组。
  - 可以直接用 NumPy 操作进行像素级处理（如 `img[100, 200] = [0, 0, 255]`）。
  - 裁剪、缩放、颜色空间转换等操作都基于数组切片和函数调用。

#### 🎯 5. 综合应用：按步骤处理图像
1. 读取图像。
2. 在图像上绘制矩形。
3. 裁剪指定区域。
4. 缩放图像。
5. 添加文字。
6. 保存结果。

> ✅ **核心洞见**：  
> **OpenCV 的强大在于其“图像即数组”的设计哲学。**  
> 你可以像操作普通数据一样操作图像，结合 NumPy 和 OpenCV API，实现从简单到复杂的各种视觉任务。

### LAB3
卷积神经网络（CNN）
#### 🔍 1. **卷积 = 滑动模板匹配**
- **操作**：用一个小窗口（卷积核）在图像上滑动。
- **计算**：窗口内像素与卷积核逐元素相乘后求和。
- **输出**：一个**响应图**，值越大表示局部区域越“像”该卷积核。
- ✅ **每个卷积核 = 一个特征探测器**（如边缘、纹理、模糊等）。

#### 🧩 2. **特征提取是分层的**
- **浅层**：提取低级特征（边缘、角点）。
- **深层**：组合低级特征，形成高级语义（眼睛、轮子、文字）。
- ✅ **多个卷积核并行工作 → 同时提取多种特征**。

#### ⚙️ 3. **仿射变换 = 特征组合器**
- 全连接层（`y = Wx + b`）对扁平化后的特征向量进行**加权组合**。
- **每行权重 `W[i, :]` = 一种“判断逻辑”**：
  - 强调某些特征（如“有胡须 + 圆眼” → 猫），
  - 抑制其他特征。
- ✅ **变换的目的不是“连接”，而是“学习如何组合特征”**。

#### 📦 4. **整体流程 = 编码 + 决策**
1. **编码阶段**（卷积 + 激活 + 池化 + 扁平化）  
   → 将原始像素压缩为**高维语义特征向量**（“原始数据库”）。
2. **决策阶段**（全连接 + 激活）  
   → 用不同“变换”对特征向量加权，做出最终判断。

#### 💡 核心洞见
> **卷积核决定“看什么”（What to look for）。卷积核的选取有很多种，每种可以对应提取原始图片的某个特征。  
> 全连接权重决定“怎么判”（How to decide）。存在很多种不同的变换（连接权重），每种变换可以通过某种运算强调原始数据的某一个特点。**  

---

### LAB4
使用 PyTorch 构建并训练神经网络

> ✅ **核心洞见**：
> **PyTorch = 神经网络的“乐高积木盒”。**
> 我们不再需要手动实现复杂的数学运算（如卷积、反向传播），而是通过**声明式**地“搭建”网络层，像拼乐高一样构建模型。PyTorch 负责所有底层的计算、优化和 GPU 加速。

#### 🧠 1. **训练的“灵魂”：损失函数与反向传播**
- **目标**：模型训练的唯一目标是**最小化损失 (Loss)**。
- **损失函数 `Loss = f(预测值, 真实值)`**：
  - 一个数学函数，用来衡量模型“猜得有多差”。
  - 分类问题常用**交叉熵损失 (Cross-Entropy Loss)**。
    - **原理**：如果模型对正确答案的预测概率很高，损失就小；反之，损失就大。
- **反向传播 (Backpropagation)**：
  - 计算出损失后，模型会“复盘”，从后往前计算出**每一个权重**对这次“犯错”应负的“责任”（即**梯度**）。
- **优化器 (Optimizer)**：
  - 得到所有权重的“责任”后，优化器（如 Adam）负责去**微调**每一个权重，让模型下次能做得更好。

#### 📦 2. **PyTorch 核心“积木” (`torch.nn` 模块)**
从现在开始，我们用 PyTorch 提供的现成“积木块”（类）来代替之前手写的函数。

##### **`nn.Conv2d` - 卷积层**
- **作用**：提取图像特征，是 CNN 的核心。
- **接口**：`nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)`
- **参数详解**：
  - `in_channels` (整数): **输入通道数**。必须与上一层的输出通道数匹配。对于第一层，它通常是 3（彩色图）。
  - `out_channels` (整数): **输出通道数**。由你**自己定义**，代表你希望这一层提取出多少种不同的特征。
  - `kernel_size` (整数或元组): 卷积核的大小，如 `3` (代表 3x3) 或 `(3, 5)`。
  - `stride` (整数): 步长。
  - `padding` (整数): 填充。
- ✅ **关键**：你只需定义“积木”的规格，PyTorch 会**自动创建并管理**内部的权重和偏置。

##### **`nn.MaxPool2d` - 最大池化层**
- **作用**：缩小特征图尺寸（下采样），减少计算量，提取最显著的特征。
- **接口**：`nn.MaxPool2d(kernel_size, stride=None)`
- **参数详解**：
  - `kernel_size`: 池化窗口的大小，如 `2` (代表 2x2)。
  - `stride`: 步长。如果省略，默认等于 `kernel_size`。

##### **`nn.LeakyReLU` - 激活函数**
- **作用**：为网络引入**非线性**，让模型能学习更复杂的模式。
- **接口**：`nn.LeakyReLU(negative_slope=0.01)`
- ✅ **用法**：通常紧跟在 `nn.Conv2d` 或 `nn.Linear` 之后。

##### **`torch.flatten` - 展平层**
- **作用**：将多维的特征图“压扁”成一个一维向量，为送入全连接层做准备。
- **接口**：`torch.flatten(x, start_dim=1)`
- **参数详解**：
  - `start_dim=1`: **极其重要！** 它的意思是“保持第 0 维（批次维度 Batch_size）不变，将从第 1 维开始的所有后续维度（C, H, W）全部展平”。
  - ✅ **这完美地处理了批处理数据，是你之前 `reshape(batch_size, -1)` 的等效实现。**

##### **`nn.Linear` - 全连接层**
- **作用**：对展平后的特征进行加权组合，做出最终的分类决策。
- **接口**：`nn.Linear(in_features, out_features)`
- **参数详解**：
  - `in_features` (整数): **输入特征数**。必须与 `flatten` 之后得到的向量长度**完全匹配**。你需要手动计算这个值。
  - `out_features` (整数): **输出特征数**。对于分类问题，它等于你的**类别总数**（比如 3 或 7）。

#### 🏗️ 3. **搭建模型的“两步走”**
所有 PyTorch 模型都通过定义一个继承自 `nn.Module` 的类来构建。

1.  **`__init__(self)` 方法 (声明积木)**
    - **作用**：这是模型的“蓝图”。在这里，你需要把你将要用到的**所有带参数的网络层**（如 `nn.Conv2d`, `nn.Linear`）都实例化，并作为类的属性（如 `self.conv1 = nn.Conv2d(...)`）。
    - ✅ **只声明，不连接。**

2.  **`forward(self, x)` 方法 (连接流水线)**
    - **作用**：这是模型的“执行流程”。它定义了当数据 `x` 输入时，应该按照怎样的**顺序**流过你在 `__init__` 中声明的那些层。
    - **返回值**：返回网络的最终输出（通常是未经 Softmax 的原始分数 Logits）。

```python
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        # 在这里声明所有需要的“积木”
        self.conv1 = nn.Conv2d(3, 64, 3)
        self.fc1 = nn.Linear(4096, 3)

    def forward(self, x):
        # 在这里定义数据如何流过这些“积木”
        x = self.conv1(x)
        # ... (经过其他层) ...
        x = self.fc1(x)
        return x
```

#### 🚀 4. **训练与推理**
- **训练 (Training)**：
  - 将模型设置为 `model.train()` 模式。
  - 循环遍历数据集，执行**前向传播、计算损失、反向传播、权重更新**。
  - 训练结束后，使用 `torch.save(model.state_dict(), 'model.pth')` 保存学到的**权重**。
- **推理 (Inference)**：
  - 创建一个和训练时**一模一样**的模型结构实例。
  - 使用 `model.load_state_dict(torch.load('model.pth'))` 加载训练好的权重。
  - **关键**：将模型设置为 `model.eval()` 模式，这会关闭 Dropout 等只在训练时使用的功能，确保预测结果的确定性。

#### ✨ 5. **模型的使用：推理 (Inference)**
> ✅ **核心洞见**：
> **推理 = 一次没有“考试”和“订正”的“模拟考”。**
> 整个过程只有**前向传播**，目的是利用已经学到的知识（权重）对新数据进行预测，不再计算损失和更新权重。

##### **💡 推理流程：从图片到答案**
1.  **加载“大脑”**：
    - 实例化一个与训练时**结构完全相同**的模型“空壳”。
    - 使用 `model.load_state_dict(torch.load('your_model.pth'))` 将训练好的权重（灵魂）加载进去。
    - **关键**：调用 `model.eval()` 将模型切换到**评估模式**。
2.  **准备“标准餐”（预处理）**：
    - 你的模型只吃特定格式的数据（如 3x48x48，归一化到 [-1, 1]）。
    - 任何新图片都必须经过**完全相同**的预处理流程（缩放、归一化、维度变换 `HWC->CHW->BCHW`）才能喂给模型。
3.  **执行预测**：
    - 将预处理好的 `Tensor` 输入模型 `output = model(input_tensor)`。
    - **关键**：将整个预测过程包裹在 `with torch.no_grad():` 代码块中，告诉 PyTorch 在此期间**不需要计算梯度**，可以节省大量计算资源。
4.  **解读“想法”（后处理）**：
    - 模型输出的是原始分数 (Logits)，通常需要进一步处理才能看懂。
    - **`torch.softmax(output, dim=1)`**：将分数转换为**概率分布**，所有值的和为 1，更具可解释性。
    - **`torch.argmax(output, dim=1)`**：直接找出分数最高的那个类别的**索引**，这是最直接的预测结果。
    - **`.item()`**：**极其重要！** 当你需要将一个**只含单个元素**的 PyTorch `Tensor`（如 `argmax` 的结果）用作 Python 的数字（如列表索引）时，必须调用 `.item()` 方法来“解包”，提取出那个纯粹的数字。

#### 🩺 6. **模型的可视化与导出 (ONNX & Netron)**
> ✅ **核心洞见**：
> **ONNX = 神经网络的“通用护照”。**
> 它是一种标准格式，能让不同框架（PyTorch, TensorFlow, Caffe...）训练出的模型互相“交流”，并被各种工具（如 Netron）所理解。

##### **`torch.onnx.export` - 制作“护照”**
- **作用**：将一个 PyTorch 模型（包含结构和权重）导出为一个 `.onnx` 文件。
- **接口**：`torch.onnx.export(model, args, f, ...)`
- **关键参数详解**：
  - `model`: 你要导出的、已经加载了权重的模型实例。
  - `args` (元组): 一个**元组**，包含一个符合模型输入尺寸的“虚拟”输入 `Tensor`。**即使只有一个输入，也必须用元组包裹**，如 `(dummy_input,)`。
  - `f` (字符串): 导出的 `.onnx` 文件名。
  - `input_names`, `output_names` (列表): (可选) 为输入输出节点命名，让可视化结果更清晰。
  - `dynamic_axes` (字典): (可选，但推荐) 声明某些维度是动态的，比如 `{'input': {0: 'batch_size'}}`，这会让导出的模型能接受不同批次大小的输入。

##### **导出流程**
1.  **先有 `.pth`**：你必须先有一个训练好的权重文件。
2.  **再写导出脚本**：
    - 在一个**新的** `.py` 文件中，重新定义你的模型 `class`。
    - 实例化模型，并加载 `.pth` 权重。
    - 调用 `torch.onnx.export()` 函数，传入模型和虚拟输入，生成 `.onnx` 文件。
    - **避坑**：`export` 功能可能需要额外的库，如 `onnx` 和 `onnxscript`，如果报错 `ModuleNotFoundError`，使用 `pip install onnx onnxscript` 安装即可。

##### **Netron - “CT扫描仪”**
- **作用**：一个图形化工具，可以打开 `.onnx` 文件，清晰地展示模型的**网络结构图、每一层的参数、以及数据在其中流动的维度变化**。
- **使用**：
  - 安装桌面版或使用网页版 ([netron.app](https://netron.app/))。
  - **打开 `.onnx` 文件**（而不是 `.pth` 文件）。Netron 会自动解析并绘制出完整的模型拓扑图。
  - 如果生成了 `.onnx.data` 文件，只需确保它和 `.onnx` 文件在**同一目录**下，Netron 会自动加载。
- ✅ **是你向他人展示和解释你模型结构的最佳工具。**

---

### LAB5

> ✅ **核心洞见**：
> **模型评估 = 给AI“体检”。**
> 训练完成不代表万事大吉。我们需要一套科学的“体检”标准（评估指标），来诊断模型到底“健康”（性能好）在哪里，“生病”（容易误判）在何处，从而指导我们如何“对症下药”（模型调优）。

#### 🩺 1. **“体检报告”的核心：评估指标 (Evaluation Metrics)**
光看总分（准确率）是不够的，我们需要更详细的“分项检查报告”。

##### **混淆矩阵 (Confusion Matrix) - “诊断底单”**
- **作用**：所有评估指标的**数据来源**。它是一个表格，清晰记录了模型在每个类别上的“功”与“过”。
  - **对角线 (TP)**：**功劳**。真实为 A，模型也预测为 A 的数量。
  - **非对角线 (FP, FN)**：**过错**。所有预测错误的样本都分布在这里。
- ✅ **核心价值**：让你一眼看出“**模型最容易把哪个类别和哪个类别搞混**”。

##### **准确率 (Accuracy) - “总分”**
- **定义**：`所有预测正确的样本 / 总样本数`
- **解读**：最直观的指标，代表模型整体的判断能力。
- **陷阱**：在**类别不均衡**（比如“开心”的图片远多于“悲伤”）时，这个“总分”会产生**严重误导**。

##### **召回率 (Recall) - “查全率”**
- **定义**：`Recall = TP / (TP + FN)` (真正例 / (真正例 + 假反例))
- **它回答的问题**：“在所有**真正**为‘开心’的图片中，我的模型成功找出了多少？”
- **解读**：衡量模型**“找得全不全”**的能力。高召回率意味着**低漏报率**。

##### **精确率 (Precision) - “查准率”**
- **定义**：`Precision = TP / (TP + FP)` (真正例 / (真正例 + 假正例))
- **它回答的问题**：“在所有**被我预测**为‘开心’的图片中，有多少是真的开心？”
- **解读**：衡量模型**“猜得准不准”**的能力。高精确率意味着**低误报率**。

##### **F1-Score - “综合分”**
- **定义**：精确率和召回率的**调和平均数**。
- **作用**：一个**平衡性指标**。只有当精确率和召回率**都比较高**时，F1 分数才会高。它比单独看 P 或 R 更能反映模型在某个类别上的综合实力。

#### 📊 2. **指标的“平均”方式：Micro vs. Macro**
- **Micro-average (微平均)**：“先合并，再计算”。
  - **逻辑**：把所有类别的 TP, FP, FN 都加起来，当作一个大的二分类问题来计算指标。
  - ✅ **`Micro-Recall` 在数值上恒等于 `Overall Accuracy`**。
- **Macro-average (宏平均)**：“先计算，再平均”。
  - **逻辑**：**平等地**对待每个类别。先算出每个类别的独立指标（如 Recall），再求它们的算术平均值。
  - ✅ **更能反映模型在样本量少的“弱势类别”上的表现**，是评估模型**均衡性**的关键指标。

#### 🛠️ 3. **模型调优“三板斧”：高级训练技巧**
> ✅ **核心洞见**：
> **调优 = 科学地“炼丹”。**
> 通过引入新的“积木”或改变“烹饪手法”，让模型学得更快、更好、更聪明。

##### **Batch Normalization (BN) - “训练稳定器”**
- **作用**：在网络层之间对数据进行归一化，强行将“跑偏”的数据分布拉回到一个稳定的“标准正态分布”上。
- **角色/目的**：
  - **加速收敛**：让训练过程更稳定，可以用更大的学习率。
  - **防止梯度问题**：缓解梯度消失/爆炸。
  - **轻微正则化**：自带一点噪声，有助于防止过拟合。
- **用法**：通常插在**卷积层之后、激活函数之前**。`Conv -> BN -> ReLU`。
- ⚠️ **避坑指南**：BN 在训练 (`.train()`) 和评估 (`.eval()`) 模式下的行为**完全不同**。加载一个不含 BN 权重的 `.pth` 文件到一个带 BN 的模型结构中，会导致评估时准确率**断崖式下跌**！**模型结构必须和权重文件完全匹配。**

##### **Dropout - “防作弊神器”**
- **作用**：在**训练时**，以一定概率 `p` 随机“丢弃”（置零）一部分神经元的输出。
- **角色/目的**：
  - **防止过拟合**：强迫网络不能依赖少数“学霸”神经元，必须学习更鲁棒、更多样化的特征组合。
  - **模拟集成**：效果类似于同时训练了成千上万个不同的“残缺”网络，提升泛化能力。
- **用法**：通常用在参数量最多的**全连接层**之后。
- ⚠️ **避坑指南**：Dropout 会**减慢模型的收敛速度**。如果添加后发现准确率下降，**首先应该尝试大幅增加训练轮次 (Epochs)**，给模型足够的时间去适应这种更困难的学习模式。

##### **Epoch & Batch Size - “火候与食材”**
- **Epoch (轮次)**：代表模型完整学习了**一遍**整个训练集。
- **Batch Size (批次大小)**：代表模型**一次**看多少张图片才更新一次权重。
  - **小 Batch**：梯度“噪声”大，有助于模型找到**泛化能力更好**的解，但训练慢，耗时。
  - **大 Batch**：梯度稳定，训练速度快，但可能陷入“尖锐”的局部最优，**泛化能力可能稍差**。

---

### LAB6

> ✅ **核心洞见**：
> **项目整合 = “组装汽车”。**
> 前面的课程，我们分别打造了“发动机”（CNN模型）、“车轮”（人脸检测）、“仪表盘”（评估指标）。现在，我们要把这些零件全部组装起来，造出一辆真正能“上路行驶”（处理真实图片/视频）的汽车，并学会用漂亮的图表（可视化）来展示它的“性能报告”。

#### 🗺️ 1. **应用整合：搭建端到端 (End-to-End) 流水线**
- **目标**：创建一个 `Detector` 类，将**人脸检测**和**情绪分类**两个独立任务**串联**起来。
- **流程**：
  1. **输入**：一张包含人脸的任意尺寸图片。
  2. **检测 (Detection)**：使用 OpenCV 的 Haar Cascade 分类器，从图片中找到所有人脸的位置 `(x, y, w, h)`。
  3. **预处理 (Preprocessing)**：对**每一个**检测到的人脸框进行裁剪、缩放 (resize to 48x48)、归一化等操作，将其处理成符合我们 `emotionNet` 模型输入要求的“标准餐”(Tensor)。
  4. **分类 (Classification)**：将预处理好的人脸 Tensor 送入我们训练好的 PyTorch 模型进行**推理 (Inference)**，得到情绪预测结果。
  5. **后处理 (Post-processing)**：将预测结果（如 "happy"）和人脸框绘制回**原始输入图片**上。
  6. **输出**：一张标注了所有人脸位置和对应情绪的图片。

#### 📊 2. **成果可视化：`Matplotlib` 的“画龙点睛”之笔**
> ✅ **核心洞见**：
> **数据可视化 = 用图讲故事。**
> 一张好的图表，胜过千言万语。它能将枯燥的数字转换成直观的洞察，清晰地展示模型的学习过程和最终性能。

##### **`plt.plot()` - 绘制训练曲线**
- **作用**：绘制**损失 (Loss)** 和**准确率 (Accuracy)** 随**训练轮次 (Epoch)** 变化的曲线图。
- **数据来源**：在**训练脚本**中，创建一个列表，在**每个 Epoch 结束时**，记录下当前在**验证集**上的 Loss 和 Accuracy。训练全部结束后，将这个列表的数据绘制出来。
- **解读**：
  - **Loss 曲线**：一条平滑下降的曲线，表明模型在不断学习和改进。
  - **Accuracy 曲线**：一条平滑上升的曲线，表明模型的分类能力在稳步提升。
  - ✅ **这是证明你的模型“学会了”的最直观证据。**

##### **`ax.matshow()` - 绘制混淆矩阵热力图**
- **作用**：将**混淆矩阵 (Confusion Matrix)** 这个二维数组，可视化成一张颜色深浅不一的**热力图 (Heatmap)**。
- **数据来源**：在**验证脚本**中，遍历整个验证集，累加预测结果，最终生成一个混-淆矩阵。
- **解读**：
  - **对角线**：颜色最深，代表**预测正确 (TP)** 的样本数量，越多越好。
  - **非对角线**：颜色越深，代表该位置对应的**误判情况越严重**。
  - ✅ **这是诊断模型“具体错在哪”的最强工具。** 比如，如果 `(True: sad, Predicted: neutral)` 这个格子的颜色很深，就说明你的模型经常把“悲伤”的表情误认为是“中性”。

##### **`plt.subplots()` - 布局管理器**
- **作用**：在一个窗口中创建多个子图（比如上下或左右排列）。
- **用法**：`fig, ax = plt.subplots(1, 2)` 创建一个 1 行 2 列的布局。`ax[0]` 和 `ax[1]` 分别控制左右两个子图。
- ✅ **让你的报告图表更紧凑、更专业。**

#### 💡 3. **Bonus 拓展：从应用到工程**

##### **Singleton (单例) 设计模式**
- **是什么**：一种保证一个类在程序运行期间**永远只有一个实例**的设计模式。
- **为什么需要**：在 `Detector` 类中，模型加载（`cv2.CascadeClassifier`, `torch.load`）是一个**非常耗时、耗内存**的操作。如果每次需要检测时都创建一个新的 `Detector` 实例，会造成巨大的资源浪费。
- **应用**：将 `Detector` 改为单例，确保整个程序无论在哪里调用它，使用的都是**同一个**已经加载好模型的实例，大大提高效率。

##### **视频处理**
- **原理**：**视频 = 连续的图片帧 (Frame)**。
- **流程**：
  1. 使用 `cv2.VideoCapture` 逐帧读取视频。
  2. 在 `while` 循环中，将**每一帧**都当作一张普通图片，送入你已经写好的 `detector.process()` 方法进行处理。
  3. 使用 `cv2.VideoWriter` 将处理后返回的、标注了结果的帧，逐一写入到一个新的视频文件中。
- ✅ **这是将你的静态图片处理能力，扩展到动态视频流的直接应用。**

---

### LAB7

**模型优化之“道”——数据增强与迁移学习**

> ✅ **核心洞-见**：
> **模型优化 = “对症下药” + “站在巨人肩膀上”。**
> 当模型性能不佳时，我们首先要通过分析（如混淆矩阵）**诊断**出问题所在（如“过拟合”或“特定类别识别差”），然后采取针对性的“药方”（如**数据增强**、**补充数据**）。最终，我们可以借助**迁移学习**，继承一个已经训练好的、强大的“通用大脑”（预训练模型），让我们的模型获得远超从零开始训练的性能。

#### 🩺 1. **诊断“过拟合” (Overfitting)**
- **症状**：**训练集**准确率很高（如 90%+），但**验证/测试集**准确率却停滞在较低水平（如 70%）。训练曲线和验证曲线之间出现了**巨大的“剪刀差”**。
- **病因**：模型过于复杂或训练数据太少，导致模型“死记硬背”了训练样本的噪声和细节，而没有学到可以泛化到新数据的通用规律。
- **处方**：**数据增强 (Data Augmentation)**。

#### 💉 2. **“药方”之一：数据增强 (Data Augmentation)**
- **作用**：“无中生有”，在保持标签不变的前提下，通过对训练图片进行随机变换，**人为地扩充训练数据集**。
- **核心思想**：强迫模型去学习**“内容”**本身，而不是图片的方向、位置或亮度。一张向左看的“开心”脸和向右看的“开心”脸，都应该被识别为“开心”。
- **常用“药材” (`torchvision.transforms`)**：
  - `RandomHorizontalFlip()`: **随机水平翻转**。对于人脸识别这类左右对称的任务，效果极好。
  - `RandomRotation(degrees)`: 在一定角度内**随机旋转**。
  - `RandomCrop(size)` & `Resize(size)`: **随机裁剪**并缩放回原尺寸。强迫模型关注图像的不同局部。
- ⚠️ **关键**：数据增强**只用于训练集 (`training`)**。评估集 (`evaluate`) 必须保持固定不变，以保证评估结果的稳定和可信。

#### 💊 3. **“药方”之二：补充特定数据 (Data Annotation)**
- **适应症**：通过混淆矩阵发现，模型**总是混淆**某两个特定的类别（如 `sad` 和 `neutral`）。
- **作用**：“缺啥补啥”。针对性地为模型表现不佳的类别，**补充更多、更多样化**的训练样本。
- **实现流程**：
  1. **收集数据**：自己录制一段做“悲伤”表情的视频。
  2. **提取样本**：用 OpenCV 的人脸检测器，从视频中逐帧提取出人脸图片并保存。
  3. **数据标注**：将这些新提取的“悲伤”脸图片，**手动**放入数据集的 `images/train/sad/` 文件夹下。
  4. **重新训练**：用扩充后的数据集重新训练模型。
- ✅ **这是提升模型性能最根本、最有效的方法。** 算法有上限，但数据的力量是无穷的。

#### 巨人肩膀：迁移学习 (Transfer Learning) 与 ResNet**
> ✅ **核心洞见**：
> **迁移学习 = 不再重复造轮子。**
> 与其从一个随机的、“一无所知”的模型开始训练，不如直接借用一个已经在数百万张图片上训练过的“通用视觉专家”（预训练模型），然后只训练它去适应我们自己的小任务。

##### **ResNet (残差网络) - 更强大的“大脑”**
- **是什么**：一种非常成功的深度 CNN 架构。
- **核心创新**：**快捷连接 (Skip Connection)**，即 `y = F(x) + x`。它允许信息“跳过”一层或多层直接向后传递。
- **作用**：解决了深度网络中的**梯度消失**问题，使得训练**非常深**的网络（几十、上百层）成为可能，从而能学习到更强大的特征表示。

##### **如何实现迁移学习**
1.  **加载预训练模型**：
    - `model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)`
    - 这一行代码会从 `torchvision` 库中，加载一个**已经用 ImageNet 数据集训练好的 ResNet18 模型**，它的权重已经包含了对世界万物（猫、狗、汽车、飞机等）的丰富视觉知识。
2.  **“换头术” - 修改分类层**：
    - 预训练的 ResNet18 最后一层（`model.fc`）是为 1000 个 ImageNet 类别设计的。我们需要把它换成我们自己的分类头。
    - `num_features = model.fc.in_features`: 首先获取原始分类头的输入特征数。
    - `model.fc = nn.Linear(num_features, 3)`: **替换！** 用一个新的、输出为 3（或 7）的全连接层，换掉原来的最后一层。
3.  **微调 (Fine-tuning)**：
    - 用你自己的数据集，以一个**较小的学习率**去训练这个“换了头”的模型。
    - **为什么用小学习率？** 因为 ResNet 的大部分层已经学得很好，我们不希望用大的学习率去“摧毁”这些宝贵的预训练知识，我们只想对权重进行**微小的调整**，让它更好地适应我们的情绪分类任务。
- ✅ **效果**：通常情况下，使用预训练模型，你可以在**更少的数据集、更短的训练时间**下，达到**远超**从零开始训练的模型的性能。

---

### LAB8
**从项目到产品——技术升级与未来展望**

> ✅ **核心洞见**：
> **完成项目不是终点，而是新的起点。**
> 我们不仅要实现功能，更要不断审视当前技术的局-限性，并积极拥抱业界更先进、更强大的解决方案（如 YOLO）。同时，要理解我们解决的问题在学术和工业界所处的位置，并思考“端到端”这一更宏大的设计哲学。

#### 🚀 1. **技术升级：从 Haar Cascade 到 YOLO——检测器的“代际革命”**

##### **Haar Cascade 分类器的局限性**
- **是什么**：一种基于传统机器学习的**模板匹配**技术。
- **缺点**：
  - **泛化能力差**：对角度、光照、遮挡非常敏感，只能识别与“模板”高度相似的人脸。
  - **性能瓶颈**：在复杂场景（如多人、小目标）下，漏检率非常高。
  - **技术过时**：已被基于深度学习的方法全面超越。

##### **YOLO (You Only Look Once) - 新一代“视觉引擎”**
- **是什么**：一个**基于深度学习**的、**端到端**的通用目标检测框架。
- **核心原理**：
  1. **网格化**：将图片划分为网格，每个网格负责预测其中心点附近的物体。
  2. **统一预测**：通过**一次**神经网络前向传播，**同时**预测出所有物体的**边界框 (Bounding Box)**、**置信度 (Confidence)** 和**类别概率 (Class Probability)**。
  3. **NMS筛选**：使用非极大值抑制算法，从海量预测框中筛选出最终结果。
- **巨大优势**：
  - **速度极快**：真正的“实时”检测。
  - **准确率高**：背景识别能力强，误检率低。
  - **泛化能力强**：对各种复杂场景（遮挡、角度、光照）的鲁棒性远超 Haar。
- ✅ **你的任务**：了解 YOLO 的优势，并**尝试使用一个预训练好的 `YOLOv5-face` 模型**，将其与你之前使用的 Haar Cascade 进行性能对比，直观感受技术的巨大进步。

#### 🎓 2. **认知回溯：了解你的数据集——FER2013**
- **是什么**：**FER2013 (Facial Expression Recognition 2013)** 是一个人脸表情识别领域的**学术基准数据集**。
- **背景**：源自顶会 ICML 2013 的一项机器学习挑战赛。
- **原始任务**：一个包含 7 个情绪类别的、更具挑战性的分类问题。
- **课程简化**：你的课程为了降低训练难度、加快训练速度，将其简化为了 3 分类问题。
- ✅ **意义**：了解这一点，能让你对自己的模型性能有一个**合理的预期**。在完整的 FER2013 上，76% 的准确率已是世界前沿水平。这让你知道，继续提升模型性能需要更先进的网络架构和训练技巧。

#### 🌌 3. **未来展望：走向“端到端” (End-to-End)**
- **你当前的方法 (两阶段, Two-Stage)**：
  1. **Stage 1**: 运行一个**检测模型**（找到脸）。
  2. **Stage 2**: 对找到的每个脸，再运行一个**分类模型**（判断表情）。
- **“端到端”的理想**：
  - 是否存在一个**单一模型**，输入一张完整的图片，就能**直接输出**所有“**人脸位置 + 对应情绪**”的结构化结果？
- **当前趋势**：
  - **大型多模态模型 (LMM)**，如 GPT-4V，已经具备了这种初步的、用自然语言描述的端到端能力。
  - **专用模型**：在工业界，更常见的是设计一个专门用于“人脸情绪检测”这个特定任务的、规模适中的端到端模型。
- ✅ **对你的启发**：这为你未来的学习和研究指明了方向。将多个独立的模块融合成一个单一的、联合优化的端到端系统，是深度学习领域一个重要且前沿的课题。

#### ✍️ 4. **项目收尾：整理与报告**
- **任务**：
  1. **整理代码**：将你从 LAB2 到 LAB7 的所有代码、模型、数据进行系统性的整理。
  2. **撰写项目报告**：用**英文**撰写一份完整的项目报告，总结你的项目目标、实现方法、实验过程、结果分析和最终结论。这是对你整个学期学习成果的最终检验。